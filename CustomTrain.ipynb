{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0fa285-12df-474e-a934-de8c455bb446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import collections\n",
    "import numbers\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "class PSMSegLoader(object):\n",
    "    def __init__(self, data_path, win_size, step, mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        self.step = step\n",
    "        self.win_size = win_size\n",
    "        self.scaler = StandardScaler()\n",
    "        data = pd.read_csv(data_path + '/train.csv')\n",
    "        data = data.values[:, 1:]\n",
    "\n",
    "        data = np.nan_to_num(data)\n",
    "\n",
    "        self.scaler.fit(data)\n",
    "        data = self.scaler.transform(data)\n",
    "        test_data = pd.read_csv(data_path + '/test.csv')\n",
    "\n",
    "        test_data = test_data.values[:, 1:]\n",
    "        test_data = np.nan_to_num(test_data)\n",
    "\n",
    "        self.test = self.scaler.transform(test_data)\n",
    "\n",
    "        self.train = data\n",
    "        self.val = self.test\n",
    "\n",
    "        self.test_labels = pd.read_csv(data_path + '/test_label.csv').values[:, 1:]\n",
    "\n",
    "        print(\"test:\", self.test.shape)\n",
    "        print(\"train:\", self.train.shape)\n",
    "        print(\"train dataset file path\", data_path + '/train.csv')\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of images in the object dataset.\n",
    "        \"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return (self.train.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'val'):\n",
    "            return (self.val.shape[0] - self.win_size) // self.step + 1\n",
    "        elif (self.mode == 'test'):\n",
    "            return (self.test.shape[0] - self.win_size) // self.step + 1\n",
    "        else:\n",
    "            return (self.test.shape[0] - self.win_size) // self.win_size + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = index * self.step\n",
    "        if self.mode == \"train\":\n",
    "            return np.float32(self.train[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'val'):\n",
    "            return np.float32(self.val[index:index + self.win_size]), np.float32(self.test_labels[0:self.win_size])\n",
    "        elif (self.mode == 'test'):\n",
    "            return np.float32(self.test[index:index + self.win_size]), np.float32(\n",
    "                self.test_labels[index:index + self.win_size])\n",
    "        else:\n",
    "            return np.float32(self.test[\n",
    "                              index // self.step * self.win_size:index // self.step * self.win_size + self.win_size]), np.float32(\n",
    "                self.test_labels[index // self.step * self.win_size:index // self.step * self.win_size + self.win_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27884a08-834a-44cb-bd08-5ebe847de1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader_segment(data_path, batch_size, win_size=100, step=100, mode='train', dataset='KDD'):\n",
    "    \n",
    "    dataset = PSMSegLoader(data_path, win_size, 1, mode)\n",
    "\n",
    "    shuffle = False\n",
    "    # if mode == 'train':\n",
    "    #     shuffle = True\n",
    "\n",
    "    data_loader = DataLoader(dataset=dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             num_workers=0)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a535414-ef63-4f57-a405-e73f6ddf638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from utils.utils import *\n",
    "from model.AnomalyTransformer import AnomalyTransformer\n",
    "\n",
    "\n",
    "def my_kl_loss(p, q):\n",
    "    res = p * (torch.log(p + 0.0001) - torch.log(q + 0.0001))\n",
    "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, lr_):\n",
    "    lr_adjust = {epoch: lr_ * (0.5 ** ((epoch - 1) // 1))}\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, dataset_name='', delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_score2 = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_loss2_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.dataset = dataset_name\n",
    "\n",
    "    def __call__(self, val_loss, val_loss2, model, path):\n",
    "        score = -val_loss\n",
    "        score2 = -val_loss2\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_score2 = score2\n",
    "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
    "        elif score < self.best_score + self.delta or score2 < self.best_score2 + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_score2 = score2\n",
    "            self.save_checkpoint(val_loss, val_loss2, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, val_loss2, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), os.path.join(path, str(self.dataset) + '_checkpoint.pth'))\n",
    "        self.val_loss_min = val_loss\n",
    "        self.val_loss2_min = val_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0bf95b-c8c5-4824-96e3-56fbb820dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(object):\n",
    "    DEFAULTS = {}\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        self.__dict__.update(Solver.DEFAULTS, **config)\n",
    "\n",
    "        self.train_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                               mode='train',\n",
    "                                               dataset=self.dataset)\n",
    "        self.vali_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='val',\n",
    "                                              dataset=self.dataset)\n",
    "        self.test_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='test',\n",
    "                                              dataset=self.dataset)\n",
    "        self.thre_loader = get_loader_segment(self.data_path, batch_size=self.batch_size, win_size=self.win_size,\n",
    "                                              mode='thre',\n",
    "                                              dataset=self.dataset)\n",
    "\n",
    "        self.build_model()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = AnomalyTransformer(win_size=self.win_size, enc_in=self.input_c, c_out=self.output_c, e_layers=3)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "\n",
    "    def vali(self, vali_loader):\n",
    "        self.model.eval()\n",
    "\n",
    "        loss_1 = []\n",
    "        loss_2 = []\n",
    "        for i, (input_data, _) in enumerate(vali_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                        prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                               self.win_size)).detach())) + torch.mean(\n",
    "                    my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)).detach(),\n",
    "                        series[u])))\n",
    "                prior_loss += (torch.mean(\n",
    "                    my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)),\n",
    "                               series[u].detach())) + torch.mean(\n",
    "                    my_kl_loss(series[u].detach(),\n",
    "                               (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)))))\n",
    "            series_loss = series_loss / len(prior)\n",
    "            prior_loss = prior_loss / len(prior)\n",
    "\n",
    "            rec_loss = self.criterion(output, input)\n",
    "            loss_1.append((rec_loss - self.k * series_loss).item())\n",
    "            loss_2.append((rec_loss + self.k * prior_loss).item())\n",
    "\n",
    "        return np.average(loss_1), np.average(loss_2)\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        print(\"======================TRAIN MODE======================\")\n",
    "\n",
    "        time_now = time.time()\n",
    "        path = self.model_save_path\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        early_stopping = EarlyStopping(patience=3, verbose=True, dataset_name=self.dataset)\n",
    "        train_steps = len(self.train_loader)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            iter_count = 0\n",
    "            loss1_list = []\n",
    "\n",
    "            epoch_time = time.time()\n",
    "            self.model.train()\n",
    "            for i, (input_data, labels) in enumerate(self.train_loader):\n",
    "\n",
    "                if i < 2:\n",
    "                    print(input_data.shape)\n",
    "                    print(input_data)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                iter_count += 1\n",
    "                input = input_data.float().to(self.device)\n",
    "\n",
    "                output, series, prior, _ = self.model(input)\n",
    "\n",
    "                # calculate Association discrepancy\n",
    "                series_loss = 0.0\n",
    "                prior_loss = 0.0\n",
    "                for u in range(len(prior)):\n",
    "                    series_loss += (torch.mean(my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach())) + torch.mean(\n",
    "                        my_kl_loss((prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                           self.win_size)).detach(),\n",
    "                                   series[u])))\n",
    "                    prior_loss += (torch.mean(my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach())) + torch.mean(\n",
    "                        my_kl_loss(series[u].detach(), (\n",
    "                                prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                       self.win_size)))))\n",
    "                series_loss = series_loss / len(prior)\n",
    "                prior_loss = prior_loss / len(prior)\n",
    "\n",
    "                rec_loss = self.criterion(output, input)\n",
    "\n",
    "                loss1_list.append((rec_loss - self.k * series_loss).item())\n",
    "                loss1 = rec_loss - self.k * series_loss\n",
    "                loss2 = rec_loss + self.k * prior_loss\n",
    "\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.num_epochs - epoch) * train_steps - i)\n",
    "                    print('loss1', loss1.item(), 'loss2', loss2.item())\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                # Minimax strategy\n",
    "                loss1.backward(retain_graph=True)\n",
    "                loss2.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(loss1_list)\n",
    "\n",
    "            vali_loss1, vali_loss2 = self.vali(self.test_loader)\n",
    "\n",
    "            print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} \".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss1))\n",
    "            early_stopping(vali_loss1, vali_loss2, self.model, path)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "            adjust_learning_rate(self.optimizer, epoch + 1, self.lr)\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(str(self.model_save_path), str(self.dataset) + '_checkpoint.pth')))\n",
    "        self.model.eval()\n",
    "        temperature = 50\n",
    "\n",
    "        print(\"======================TEST MODE======================\")\n",
    "\n",
    "        criterion = nn.MSELoss(reduce=False)\n",
    "\n",
    "        # (1) stastic on the train set\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.train_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        train_energy = np.array(attens_energy)\n",
    "\n",
    "        # (2) find the threshold\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "            # Metric\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        test_energy = np.array(attens_energy)\n",
    "        combined_energy = np.concatenate([train_energy, test_energy], axis=0)\n",
    "        thresh = np.percentile(combined_energy, 100 - self.anormly_ratio)\n",
    "        print(\"Threshold :\", thresh)\n",
    "\n",
    "        # (3) evaluation on the test set\n",
    "        test_labels = []\n",
    "        attens_energy = []\n",
    "        for i, (input_data, labels) in enumerate(self.thre_loader):\n",
    "            input = input_data.float().to(self.device)\n",
    "            output, series, prior, _ = self.model(input)\n",
    "\n",
    "            loss = torch.mean(criterion(input, output), dim=-1)\n",
    "\n",
    "            series_loss = 0.0\n",
    "            prior_loss = 0.0\n",
    "            for u in range(len(prior)):\n",
    "                if u == 0:\n",
    "                    series_loss = my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss = my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "                else:\n",
    "                    series_loss += my_kl_loss(series[u], (\n",
    "                            prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                   self.win_size)).detach()) * temperature\n",
    "                    prior_loss += my_kl_loss(\n",
    "                        (prior[u] / torch.unsqueeze(torch.sum(prior[u], dim=-1), dim=-1).repeat(1, 1, 1,\n",
    "                                                                                                self.win_size)),\n",
    "                        series[u].detach()) * temperature\n",
    "            metric = torch.softmax((-series_loss - prior_loss), dim=-1)\n",
    "\n",
    "            cri = metric * loss\n",
    "            cri = cri.detach().cpu().numpy()\n",
    "            attens_energy.append(cri)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "        attens_energy = np.concatenate(attens_energy, axis=0).reshape(-1)\n",
    "        test_labels = np.concatenate(test_labels, axis=0).reshape(-1)\n",
    "        test_energy = np.array(attens_energy)\n",
    "        test_labels = np.array(test_labels)\n",
    "\n",
    "        pred = (test_energy > thresh).astype(int)\n",
    "\n",
    "        gt = test_labels.astype(int)\n",
    "\n",
    "        print(\"pred:   \", pred.shape)\n",
    "        print(\"gt:     \", gt.shape)\n",
    "\n",
    "        # detection adjustment: please see this issue for more information https://github.com/thuml/Anomaly-Transformer/issues/14\n",
    "        anomaly_state = False\n",
    "        for i in range(len(gt)):\n",
    "            if gt[i] == 1 and pred[i] == 1 and not anomaly_state:\n",
    "                anomaly_state = True\n",
    "                for j in range(i, 0, -1):\n",
    "                    if gt[j] == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        if pred[j] == 0:\n",
    "                            pred[j] = 1\n",
    "                for j in range(i, len(gt)):\n",
    "                    if gt[j] == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        if pred[j] == 0:\n",
    "                            pred[j] = 1\n",
    "            elif gt[i] == 0:\n",
    "                anomaly_state = False\n",
    "            if anomaly_state:\n",
    "                pred[i] = 1\n",
    "\n",
    "        pred = np.array(pred)\n",
    "        gt = np.array(gt)\n",
    "        print(\"pred: \", pred.shape)\n",
    "        print(\"gt:   \", gt.shape)\n",
    "\n",
    "        from sklearn.metrics import precision_recall_fscore_support\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(gt, pred)\n",
    "        precision, recall, f_score, support = precision_recall_fscore_support(gt, pred,\n",
    "                                                                              average='binary')\n",
    "        print(\n",
    "            \"Accuracy : {:0.4f}, Precision : {:0.4f}, Recall : {:0.4f}, F-score : {:0.4f} \".format(\n",
    "                accuracy, precision,\n",
    "                recall, f_score))\n",
    "\n",
    "        return accuracy, precision, recall, f_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b736fb-5cbd-4cb0-a676-e0713c0f2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    return v.lower() in ('true')\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    cudnn.benchmark = True\n",
    "    if (not os.path.exists(config.model_save_path)):\n",
    "        mkdir(config.model_save_path)\n",
    "    solver = Solver(vars(config))\n",
    "\n",
    "    if config.mode == 'train':\n",
    "        solver.train()\n",
    "    elif config.mode == 'test':\n",
    "        solver.test()\n",
    "\n",
    "    return solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69675357-2ec3-48ba-a7d6-91e06f0044d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0001,\n",
       " 'num_epochs': 5,\n",
       " 'k': 3,\n",
       " 'win_size': 100,\n",
       " 'input_c': 25,\n",
       " 'output_c': 25,\n",
       " 'batch_size': 256,\n",
       " 'pretrained_model': None,\n",
       " 'dataset': 'PSM',\n",
       " 'mode': 'train',\n",
       " 'data_path': 'dataset/PSM',\n",
       " 'model_save_path': 'checkpoints',\n",
       " 'anormly_ratio': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "from torch.backends import cudnn\n",
    "from utils.utils import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--num_epochs', type=int, default=5)\n",
    "parser.add_argument('--k', type=int, default=3)\n",
    "parser.add_argument('--win_size', type=int, default=100)\n",
    "parser.add_argument('--input_c', type=int, default=25)\n",
    "parser.add_argument('--output_c', type=int, default=25)\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--pretrained_model', type=str, default=None)\n",
    "parser.add_argument('--dataset', type=str, default='PSM')\n",
    "parser.add_argument('--mode', type=str, default='train', choices=['train', 'test'])\n",
    "parser.add_argument('--data_path', type=str, default='dataset/PSM')\n",
    "parser.add_argument('--model_save_path', type=str, default='checkpoints')\n",
    "parser.add_argument('--anormly_ratio', type=float, default=1.0)\n",
    "\n",
    "config = parser.parse_args(args=[])\n",
    "args = vars(config)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "498991e8-7f23-43e1-9663-5c7cd4e3acaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: (87841, 25)\n",
      "train: (132481, 25)\n",
      "train dataset file path dataset/PSM/train.csv\n",
      "test: (87841, 25)\n",
      "train: (132481, 25)\n",
      "train dataset file path dataset/PSM/train.csv\n",
      "test: (87841, 25)\n",
      "train: (132481, 25)\n",
      "train dataset file path dataset/PSM/train.csv\n",
      "test: (87841, 25)\n",
      "train: (132481, 25)\n",
      "train dataset file path dataset/PSM/train.csv\n",
      "======================TRAIN MODE======================\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.5317, -1.9877, -0.2659,  ...,  2.2652, -0.7976, -0.4634],\n",
      "         [-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         ...,\n",
      "         [-1.4387, -1.9398, -0.2025,  ...,  2.0419, -0.7802, -0.4634],\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         [-1.5302, -1.9921, -0.2666,  ...,  2.1652, -0.7976, -0.0089],\n",
      "         ...,\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284],\n",
      "         [-1.4231, -1.9653, -0.1961,  ...,  2.1037, -0.7802, -0.5933]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2011, -1.8986, -0.1207,  ...,  2.0282, -0.7627, -0.5284],\n",
      "         [-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         ...,\n",
      "         [-1.0847, -1.8202, -0.0563,  ...,  2.0818, -0.7453, -0.2686],\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881]],\n",
      "\n",
      "        [[-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634]],\n",
      "\n",
      "        [[-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         ...,\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686]]])\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284]],\n",
      "\n",
      "        [[-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686]],\n",
      "\n",
      "        [[-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         [-1.1970, -1.8963, -0.1063,  ...,  2.1558, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686],\n",
      "         [-1.0859, -1.8258, -0.0586,  ...,  2.0051, -0.6582, -0.1388]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9184, -1.7548,  0.0309,  ...,  2.0519, -0.7976,  0.1859],\n",
      "         [-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         ...,\n",
      "         [-0.8101, -1.6940,  0.0503,  ...,  2.1808, -0.7802,  0.9651],\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508]],\n",
      "\n",
      "        [[-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         ...,\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560]],\n",
      "\n",
      "        [[-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         [-0.9036, -1.7706,  0.0318,  ...,  1.9065, -0.7279, -0.0738],\n",
      "         ...,\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560],\n",
      "         [-0.8081, -1.7113,  0.0315,  ...,  2.1645, -0.7802, -0.2686]]])\n",
      "loss1 -20.73957633972168 loss2 21.20957374572754\n",
      "\tspeed: 0.4385s/iter; left time: 1131.8501s\n",
      "loss1 -20.599374771118164 loss2 21.20912742614746\n",
      "\tspeed: 0.4232s/iter; left time: 1087.9737s\n",
      "loss1 -23.00893783569336 loss2 23.33163070678711\n",
      "\tspeed: 0.4232s/iter; left time: 1083.9254s\n",
      "loss1 -27.288494110107422 loss2 27.81676483154297\n",
      "\tspeed: 0.4233s/iter; left time: 1079.7584s\n",
      "loss1 -32.1619987487793 loss2 32.48125076293945\n",
      "\tspeed: 0.4233s/iter; left time: 1075.7065s\n",
      "loss1 -35.80167770385742 loss2 36.14230728149414\n",
      "\tspeed: 0.4234s/iter; left time: 1071.6632s\n",
      "loss1 -38.33522033691406 loss2 38.60038757324219\n",
      "\tspeed: 0.4234s/iter; left time: 1067.4466s\n",
      "loss1 -42.008453369140625 loss2 42.299713134765625\n",
      "\tspeed: 0.4234s/iter; left time: 1063.1732s\n",
      "loss1 -43.87065505981445 loss2 44.09006881713867\n",
      "\tspeed: 0.4234s/iter; left time: 1058.8383s\n",
      "loss1 -43.9366455078125 loss2 44.24531555175781\n",
      "\tspeed: 0.4234s/iter; left time: 1054.7646s\n",
      "loss1 -42.72108459472656 loss2 42.992393493652344\n",
      "\tspeed: 0.4235s/iter; left time: 1050.6408s\n",
      "loss1 -43.91813278198242 loss2 44.11931228637695\n",
      "\tspeed: 0.4236s/iter; left time: 1046.5991s\n",
      "loss1 -44.90471649169922 loss2 45.06603240966797\n",
      "\tspeed: 0.4235s/iter; left time: 1042.2132s\n",
      "loss1 -45.03760528564453 loss2 45.146507263183594\n",
      "\tspeed: 0.4235s/iter; left time: 1037.9389s\n",
      "loss1 -44.9133415222168 loss2 45.281314849853516\n",
      "\tspeed: 0.4235s/iter; left time: 1033.6473s\n",
      "loss1 -45.3221549987793 loss2 45.89719009399414\n",
      "\tspeed: 0.4236s/iter; left time: 1029.6629s\n",
      "loss1 -45.432552337646484 loss2 45.6614875793457\n",
      "\tspeed: 0.4234s/iter; left time: 1025.1511s\n",
      "loss1 -46.20421600341797 loss2 46.63134002685547\n",
      "\tspeed: 0.4234s/iter; left time: 1020.9357s\n",
      "loss1 -45.95985794067383 loss2 46.10714340209961\n",
      "\tspeed: 0.4235s/iter; left time: 1016.8446s\n",
      "loss1 -45.966121673583984 loss2 46.21615982055664\n",
      "\tspeed: 0.4235s/iter; left time: 1012.5506s\n",
      "loss1 -46.722957611083984 loss2 46.89499282836914\n",
      "\tspeed: 0.4235s/iter; left time: 1008.2844s\n",
      "loss1 -46.946678161621094 loss2 47.134132385253906\n",
      "\tspeed: 0.4235s/iter; left time: 1004.0174s\n",
      "loss1 -46.421607971191406 loss2 47.038185119628906\n",
      "\tspeed: 0.4235s/iter; left time: 999.8899s\n",
      "loss1 -46.85645294189453 loss2 47.048500061035156\n",
      "\tspeed: 0.4236s/iter; left time: 995.8000s\n",
      "loss1 -46.990211486816406 loss2 47.115013122558594\n",
      "\tspeed: 0.4236s/iter; left time: 991.6036s\n",
      "loss1 -47.00883865356445 loss2 47.18931198120117\n",
      "\tspeed: 0.4236s/iter; left time: 987.3095s\n",
      "loss1 -46.995601654052734 loss2 47.35243606567383\n",
      "\tspeed: 0.4236s/iter; left time: 983.1676s\n",
      "loss1 -46.746212005615234 loss2 47.47116470336914\n",
      "\tspeed: 0.4236s/iter; left time: 978.8579s\n",
      "loss1 -47.18157958984375 loss2 47.352020263671875\n",
      "\tspeed: 0.4235s/iter; left time: 974.4160s\n",
      "loss1 -47.0327262878418 loss2 47.24409866333008\n",
      "\tspeed: 0.4235s/iter; left time: 970.3215s\n",
      "loss1 -46.953792572021484 loss2 47.18423843383789\n",
      "\tspeed: 0.4235s/iter; left time: 966.0909s\n",
      "loss1 -46.75693130493164 loss2 46.88532638549805\n",
      "\tspeed: 0.4235s/iter; left time: 961.8415s\n",
      "loss1 -46.9511604309082 loss2 47.06980514526367\n",
      "\tspeed: 0.4236s/iter; left time: 957.6713s\n",
      "loss1 -47.497718811035156 loss2 47.62275695800781\n",
      "\tspeed: 0.4235s/iter; left time: 953.2029s\n",
      "loss1 -47.459808349609375 loss2 47.726951599121094\n",
      "\tspeed: 0.4236s/iter; left time: 949.3099s\n",
      "loss1 -47.42281723022461 loss2 47.64250564575195\n",
      "\tspeed: 0.4236s/iter; left time: 944.9782s\n",
      "loss1 -47.254764556884766 loss2 47.46110916137695\n",
      "\tspeed: 0.4236s/iter; left time: 940.7138s\n",
      "loss1 -47.490028381347656 loss2 47.69811248779297\n",
      "\tspeed: 0.4235s/iter; left time: 936.4073s\n",
      "loss1 -47.23964309692383 loss2 47.556888580322266\n",
      "\tspeed: 0.4235s/iter; left time: 932.2118s\n",
      "loss1 -47.130950927734375 loss2 47.398773193359375\n",
      "\tspeed: 0.4235s/iter; left time: 927.8932s\n",
      "loss1 -47.52956008911133 loss2 47.64301681518555\n",
      "\tspeed: 0.4235s/iter; left time: 923.7027s\n",
      "loss1 -47.234989166259766 loss2 47.84688949584961\n",
      "\tspeed: 0.4235s/iter; left time: 919.5097s\n",
      "loss1 -47.17789840698242 loss2 47.66499710083008\n",
      "\tspeed: 0.4235s/iter; left time: 915.2789s\n",
      "loss1 -47.66469955444336 loss2 47.83627700805664\n",
      "\tspeed: 0.4235s/iter; left time: 910.9705s\n",
      "loss1 -47.61481857299805 loss2 47.80344009399414\n",
      "\tspeed: 0.4235s/iter; left time: 906.7346s\n",
      "loss1 -47.37944030761719 loss2 47.98564147949219\n",
      "\tspeed: 0.4234s/iter; left time: 902.2852s\n",
      "loss1 -47.59885025024414 loss2 47.79166793823242\n",
      "\tspeed: 0.4235s/iter; left time: 898.1883s\n",
      "loss1 -47.14918518066406 loss2 47.46904754638672\n",
      "\tspeed: 0.4235s/iter; left time: 894.0142s\n",
      "loss1 -47.6988410949707 loss2 47.922115325927734\n",
      "\tspeed: 0.4235s/iter; left time: 889.7530s\n",
      "loss1 -47.610801696777344 loss2 47.944976806640625\n",
      "\tspeed: 0.4235s/iter; left time: 885.5249s\n",
      "loss1 -47.834678649902344 loss2 47.98467254638672\n",
      "\tspeed: 0.4235s/iter; left time: 881.3181s\n",
      "Epoch: 1 cost time: 219.5991656780243\n",
      "Epoch: 1, Steps: 518 | Train Loss: -43.6908439 Vali Loss: -47.0178934 \n",
      "Validation loss decreased (inf --> -47.017893).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.5317, -1.9877, -0.2659,  ...,  2.2652, -0.7976, -0.4634],\n",
      "         [-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         ...,\n",
      "         [-1.4387, -1.9398, -0.2025,  ...,  2.0419, -0.7802, -0.4634],\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         [-1.5302, -1.9921, -0.2666,  ...,  2.1652, -0.7976, -0.0089],\n",
      "         ...,\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284],\n",
      "         [-1.4231, -1.9653, -0.1961,  ...,  2.1037, -0.7802, -0.5933]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2011, -1.8986, -0.1207,  ...,  2.0282, -0.7627, -0.5284],\n",
      "         [-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         ...,\n",
      "         [-1.0847, -1.8202, -0.0563,  ...,  2.0818, -0.7453, -0.2686],\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881]],\n",
      "\n",
      "        [[-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634]],\n",
      "\n",
      "        [[-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         ...,\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686]]])\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284]],\n",
      "\n",
      "        [[-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686]],\n",
      "\n",
      "        [[-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         [-1.1970, -1.8963, -0.1063,  ...,  2.1558, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686],\n",
      "         [-1.0859, -1.8258, -0.0586,  ...,  2.0051, -0.6582, -0.1388]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9184, -1.7548,  0.0309,  ...,  2.0519, -0.7976,  0.1859],\n",
      "         [-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         ...,\n",
      "         [-0.8101, -1.6940,  0.0503,  ...,  2.1808, -0.7802,  0.9651],\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508]],\n",
      "\n",
      "        [[-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         ...,\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560]],\n",
      "\n",
      "        [[-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         [-0.9036, -1.7706,  0.0318,  ...,  1.9065, -0.7279, -0.0738],\n",
      "         ...,\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560],\n",
      "         [-0.8081, -1.7113,  0.0315,  ...,  2.1645, -0.7802, -0.2686]]])\n",
      "loss1 -47.55009460449219 loss2 47.74208068847656\n",
      "\tspeed: 5.8992s/iter; left time: 12170.0078s\n",
      "loss1 -47.81904983520508 loss2 47.88801193237305\n",
      "\tspeed: 0.4234s/iter; left time: 869.2657s\n",
      "loss1 -47.579891204833984 loss2 47.7520637512207\n",
      "\tspeed: 0.4234s/iter; left time: 864.9568s\n",
      "loss1 -47.60102462768555 loss2 47.753604888916016\n",
      "\tspeed: 0.4234s/iter; left time: 860.7631s\n",
      "loss1 -47.701637268066406 loss2 47.78508758544922\n",
      "\tspeed: 0.4234s/iter; left time: 856.4597s\n",
      "loss1 -47.82477951049805 loss2 47.92229080200195\n",
      "\tspeed: 0.4234s/iter; left time: 852.3226s\n",
      "loss1 -47.91489028930664 loss2 47.971981048583984\n",
      "\tspeed: 0.4234s/iter; left time: 847.9752s\n",
      "loss1 -47.743247985839844 loss2 47.871421813964844\n",
      "\tspeed: 0.4234s/iter; left time: 843.8085s\n",
      "loss1 -47.82625961303711 loss2 47.94887161254883\n",
      "\tspeed: 0.4234s/iter; left time: 839.5456s\n",
      "loss1 -47.910518646240234 loss2 48.01631546020508\n",
      "\tspeed: 0.4233s/iter; left time: 835.1829s\n",
      "loss1 -47.80057907104492 loss2 47.904788970947266\n",
      "\tspeed: 0.4234s/iter; left time: 831.0390s\n",
      "loss1 -47.860111236572266 loss2 47.943111419677734\n",
      "\tspeed: 0.4233s/iter; left time: 826.6633s\n",
      "loss1 -47.76679611206055 loss2 47.82634353637695\n",
      "\tspeed: 0.4234s/iter; left time: 822.5698s\n",
      "loss1 -47.586952209472656 loss2 47.63031005859375\n",
      "\tspeed: 0.4233s/iter; left time: 818.2467s\n",
      "loss1 -47.80684280395508 loss2 48.02499771118164\n",
      "\tspeed: 0.4232s/iter; left time: 813.8085s\n",
      "loss1 -47.85548782348633 loss2 48.0295524597168\n",
      "\tspeed: 0.4234s/iter; left time: 810.0085s\n",
      "loss1 -47.801422119140625 loss2 47.89190673828125\n",
      "\tspeed: 0.4234s/iter; left time: 805.6482s\n",
      "loss1 -47.8115119934082 loss2 48.010494232177734\n",
      "\tspeed: 0.4233s/iter; left time: 801.3257s\n",
      "loss1 -47.785072326660156 loss2 47.846656799316406\n",
      "\tspeed: 0.4232s/iter; left time: 796.9508s\n",
      "loss1 -47.90421676635742 loss2 48.0064582824707\n",
      "\tspeed: 0.4233s/iter; left time: 792.8701s\n",
      "loss1 -47.99193572998047 loss2 48.082313537597656\n",
      "\tspeed: 0.4233s/iter; left time: 788.6708s\n",
      "loss1 -47.98798751831055 loss2 48.09859848022461\n",
      "\tspeed: 0.4232s/iter; left time: 784.1883s\n",
      "loss1 -47.86273193359375 loss2 48.001251220703125\n",
      "\tspeed: 0.4232s/iter; left time: 779.9914s\n",
      "loss1 -47.914710998535156 loss2 47.979400634765625\n",
      "\tspeed: 0.4234s/iter; left time: 776.0232s\n",
      "loss1 -48.04148864746094 loss2 48.09417724609375\n",
      "\tspeed: 0.4232s/iter; left time: 771.4139s\n",
      "loss1 -48.02875900268555 loss2 48.12507247924805\n",
      "\tspeed: 0.4232s/iter; left time: 767.2650s\n",
      "loss1 -47.883548736572266 loss2 48.10213851928711\n",
      "\tspeed: 0.4232s/iter; left time: 763.0324s\n",
      "loss1 -47.8525276184082 loss2 48.10496139526367\n",
      "\tspeed: 0.4232s/iter; left time: 758.8545s\n",
      "loss1 -48.072418212890625 loss2 48.143951416015625\n",
      "\tspeed: 0.4232s/iter; left time: 754.5735s\n",
      "loss1 -47.91464614868164 loss2 48.025325775146484\n",
      "\tspeed: 0.4232s/iter; left time: 750.3534s\n",
      "loss1 -47.96144485473633 loss2 48.07249069213867\n",
      "\tspeed: 0.4232s/iter; left time: 746.0906s\n",
      "loss1 -47.860801696777344 loss2 47.931968688964844\n",
      "\tspeed: 0.4232s/iter; left time: 741.9346s\n",
      "loss1 -47.96760559082031 loss2 48.03300476074219\n",
      "\tspeed: 0.4233s/iter; left time: 737.8809s\n",
      "loss1 -48.149356842041016 loss2 48.23060989379883\n",
      "\tspeed: 0.4232s/iter; left time: 733.4608s\n",
      "loss1 -48.02774429321289 loss2 48.13950729370117\n",
      "\tspeed: 0.4233s/iter; left time: 729.4024s\n",
      "loss1 -48.026214599609375 loss2 48.11412048339844\n",
      "\tspeed: 0.4234s/iter; left time: 725.2109s\n",
      "loss1 -48.101585388183594 loss2 48.20117950439453\n",
      "\tspeed: 0.4234s/iter; left time: 721.0564s\n",
      "loss1 -48.001136779785156 loss2 48.112876892089844\n",
      "\tspeed: 0.4233s/iter; left time: 716.7071s\n",
      "loss1 -47.970821380615234 loss2 48.16141128540039\n",
      "\tspeed: 0.4233s/iter; left time: 712.4864s\n",
      "loss1 -47.953041076660156 loss2 48.04547119140625\n",
      "\tspeed: 0.4232s/iter; left time: 708.0431s\n",
      "loss1 -48.09465026855469 loss2 48.17407989501953\n",
      "\tspeed: 0.4233s/iter; left time: 703.9177s\n",
      "loss1 -47.79582977294922 loss2 47.9853515625\n",
      "\tspeed: 0.4233s/iter; left time: 699.7108s\n",
      "loss1 -47.953670501708984 loss2 48.19498825073242\n",
      "\tspeed: 0.4234s/iter; left time: 695.6308s\n",
      "loss1 -48.19028091430664 loss2 48.300785064697266\n",
      "\tspeed: 0.4233s/iter; left time: 691.2957s\n",
      "loss1 -48.142616271972656 loss2 48.28162384033203\n",
      "\tspeed: 0.4233s/iter; left time: 687.0537s\n",
      "loss1 -48.04718780517578 loss2 48.259124755859375\n",
      "\tspeed: 0.4233s/iter; left time: 682.8180s\n",
      "loss1 -48.14714431762695 loss2 48.26701736450195\n",
      "\tspeed: 0.4234s/iter; left time: 678.6642s\n",
      "loss1 -47.53294372558594 loss2 47.834869384765625\n",
      "\tspeed: 0.4231s/iter; left time: 673.9369s\n",
      "loss1 -48.11656951904297 loss2 48.246185302734375\n",
      "\tspeed: 0.4231s/iter; left time: 669.7393s\n",
      "loss1 -48.06169509887695 loss2 48.14718246459961\n",
      "\tspeed: 0.4230s/iter; left time: 665.4186s\n",
      "loss1 -48.113468170166016 loss2 48.16677474975586\n",
      "\tspeed: 0.4229s/iter; left time: 661.0689s\n",
      "Epoch: 2 cost time: 218.87301635742188\n",
      "Epoch: 2, Steps: 518 | Train Loss: -47.8625206 Vali Loss: -47.7572755 \n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.5317, -1.9877, -0.2659,  ...,  2.2652, -0.7976, -0.4634],\n",
      "         [-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         ...,\n",
      "         [-1.4387, -1.9398, -0.2025,  ...,  2.0419, -0.7802, -0.4634],\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         [-1.5302, -1.9921, -0.2666,  ...,  2.1652, -0.7976, -0.0089],\n",
      "         ...,\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284],\n",
      "         [-1.4231, -1.9653, -0.1961,  ...,  2.1037, -0.7802, -0.5933]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2011, -1.8986, -0.1207,  ...,  2.0282, -0.7627, -0.5284],\n",
      "         [-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         ...,\n",
      "         [-1.0847, -1.8202, -0.0563,  ...,  2.0818, -0.7453, -0.2686],\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881]],\n",
      "\n",
      "        [[-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634]],\n",
      "\n",
      "        [[-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         ...,\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686]]])\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284]],\n",
      "\n",
      "        [[-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686]],\n",
      "\n",
      "        [[-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         [-1.1970, -1.8963, -0.1063,  ...,  2.1558, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686],\n",
      "         [-1.0859, -1.8258, -0.0586,  ...,  2.0051, -0.6582, -0.1388]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9184, -1.7548,  0.0309,  ...,  2.0519, -0.7976,  0.1859],\n",
      "         [-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         ...,\n",
      "         [-0.8101, -1.6940,  0.0503,  ...,  2.1808, -0.7802,  0.9651],\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508]],\n",
      "\n",
      "        [[-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         ...,\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560]],\n",
      "\n",
      "        [[-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         [-0.9036, -1.7706,  0.0318,  ...,  1.9065, -0.7279, -0.0738],\n",
      "         ...,\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560],\n",
      "         [-0.8081, -1.7113,  0.0315,  ...,  2.1645, -0.7802, -0.2686]]])\n",
      "loss1 -47.971900939941406 loss2 48.05284881591797\n",
      "\tspeed: 5.8527s/iter; left time: 9042.4816s\n",
      "loss1 -48.144989013671875 loss2 48.208282470703125\n",
      "\tspeed: 0.4230s/iter; left time: 649.2628s\n",
      "loss1 -48.06156539916992 loss2 48.14603042602539\n",
      "\tspeed: 0.4229s/iter; left time: 644.9804s\n",
      "loss1 -47.9761848449707 loss2 48.064327239990234\n",
      "\tspeed: 0.4228s/iter; left time: 640.5841s\n",
      "loss1 -48.0521240234375 loss2 48.096717834472656\n",
      "\tspeed: 0.4228s/iter; left time: 636.3804s\n",
      "loss1 -48.108787536621094 loss2 48.154197692871094\n",
      "\tspeed: 0.4228s/iter; left time: 632.1480s\n",
      "loss1 -48.3304443359375 loss2 48.36405944824219\n",
      "\tspeed: 0.4230s/iter; left time: 628.1782s\n",
      "loss1 -48.06524658203125 loss2 48.130882263183594\n",
      "\tspeed: 0.4230s/iter; left time: 623.8612s\n",
      "loss1 -48.06914138793945 loss2 48.12751388549805\n",
      "\tspeed: 0.4229s/iter; left time: 619.5456s\n",
      "loss1 -48.1872444152832 loss2 48.23714828491211\n",
      "\tspeed: 0.4227s/iter; left time: 615.0254s\n",
      "loss1 -48.211944580078125 loss2 48.26318359375\n",
      "\tspeed: 0.4227s/iter; left time: 610.8443s\n",
      "loss1 -48.22824478149414 loss2 48.26057052612305\n",
      "\tspeed: 0.4227s/iter; left time: 606.6165s\n",
      "loss1 -48.14634323120117 loss2 48.18112564086914\n",
      "\tspeed: 0.4227s/iter; left time: 602.3792s\n",
      "loss1 -47.97650146484375 loss2 48.006370544433594\n",
      "\tspeed: 0.4227s/iter; left time: 598.1674s\n",
      "loss1 -48.260108947753906 loss2 48.397422790527344\n",
      "\tspeed: 0.4227s/iter; left time: 593.8705s\n",
      "loss1 -48.256309509277344 loss2 48.38127899169922\n",
      "\tspeed: 0.4229s/iter; left time: 589.9774s\n",
      "loss1 -48.16544723510742 loss2 48.2125129699707\n",
      "\tspeed: 0.4228s/iter; left time: 585.5502s\n",
      "loss1 -48.10603713989258 loss2 48.25163650512695\n",
      "\tspeed: 0.4228s/iter; left time: 581.3277s\n",
      "loss1 -48.18792724609375 loss2 48.22491455078125\n",
      "\tspeed: 0.4228s/iter; left time: 577.1782s\n",
      "loss1 -48.2042350769043 loss2 48.26362228393555\n",
      "\tspeed: 0.4229s/iter; left time: 572.9954s\n",
      "loss1 -48.22850799560547 loss2 48.259376525878906\n",
      "\tspeed: 0.4228s/iter; left time: 568.6029s\n",
      "loss1 -48.31827163696289 loss2 48.357547760009766\n",
      "\tspeed: 0.4228s/iter; left time: 564.4720s\n",
      "loss1 -48.17768096923828 loss2 48.239601135253906\n",
      "\tspeed: 0.4229s/iter; left time: 560.2991s\n",
      "loss1 -48.26762771606445 loss2 48.30685043334961\n",
      "\tspeed: 0.4229s/iter; left time: 556.0947s\n",
      "loss1 -48.30628204345703 loss2 48.33905792236328\n",
      "\tspeed: 0.4228s/iter; left time: 551.7191s\n",
      "loss1 -48.286319732666016 loss2 48.33855056762695\n",
      "\tspeed: 0.4228s/iter; left time: 547.5424s\n",
      "loss1 -48.1658821105957 loss2 48.325321197509766\n",
      "\tspeed: 0.4229s/iter; left time: 543.3686s\n",
      "loss1 -48.139225006103516 loss2 48.241336822509766\n",
      "\tspeed: 0.4228s/iter; left time: 539.0636s\n",
      "loss1 -48.27995300292969 loss2 48.320526123046875\n",
      "\tspeed: 0.4228s/iter; left time: 534.8861s\n",
      "loss1 -48.09519577026367 loss2 48.16300582885742\n",
      "\tspeed: 0.4228s/iter; left time: 530.5933s\n",
      "loss1 -48.33139419555664 loss2 48.39706039428711\n",
      "\tspeed: 0.4228s/iter; left time: 526.4003s\n",
      "loss1 -48.18583297729492 loss2 48.219852447509766\n",
      "\tspeed: 0.4229s/iter; left time: 522.2862s\n",
      "loss1 -48.32867431640625 loss2 48.37095642089844\n",
      "\tspeed: 0.4230s/iter; left time: 518.1919s\n",
      "loss1 -48.434471130371094 loss2 48.482826232910156\n",
      "\tspeed: 0.4229s/iter; left time: 513.8415s\n",
      "loss1 -48.30764389038086 loss2 48.353572845458984\n",
      "\tspeed: 0.4230s/iter; left time: 509.6885s\n",
      "loss1 -48.2974967956543 loss2 48.35575485229492\n",
      "\tspeed: 0.4229s/iter; left time: 505.3982s\n",
      "loss1 -48.35586929321289 loss2 48.413150787353516\n",
      "\tspeed: 0.4231s/iter; left time: 501.3561s\n",
      "loss1 -48.291595458984375 loss2 48.35630798339844\n",
      "\tspeed: 0.4230s/iter; left time: 497.0046s\n",
      "loss1 -48.22220230102539 loss2 48.29726791381836\n",
      "\tspeed: 0.4230s/iter; left time: 492.7768s\n",
      "loss1 -48.344120025634766 loss2 48.38364791870117\n",
      "\tspeed: 0.4230s/iter; left time: 488.5378s\n",
      "loss1 -48.33747482299805 loss2 48.39133071899414\n",
      "\tspeed: 0.4228s/iter; left time: 484.1513s\n",
      "loss1 -47.94206237792969 loss2 48.120513916015625\n",
      "\tspeed: 0.4232s/iter; left time: 480.3502s\n",
      "loss1 -48.24929428100586 loss2 48.41086959838867\n",
      "\tspeed: 0.4231s/iter; left time: 475.9597s\n",
      "loss1 -48.49055099487305 loss2 48.55294418334961\n",
      "\tspeed: 0.4229s/iter; left time: 471.5140s\n",
      "loss1 -48.45781707763672 loss2 48.51781463623047\n",
      "\tspeed: 0.4230s/iter; left time: 467.4646s\n",
      "loss1 -48.34678649902344 loss2 48.47554016113281\n",
      "\tspeed: 0.4232s/iter; left time: 463.3519s\n",
      "loss1 -48.46909713745117 loss2 48.54680252075195\n",
      "\tspeed: 0.4232s/iter; left time: 459.1914s\n",
      "loss1 -48.301334381103516 loss2 48.41769027709961\n",
      "\tspeed: 0.4227s/iter; left time: 454.4259s\n",
      "loss1 -48.45955276489258 loss2 48.52322006225586\n",
      "\tspeed: 0.4227s/iter; left time: 450.1319s\n",
      "loss1 -48.37439727783203 loss2 48.435157775878906\n",
      "\tspeed: 0.4228s/iter; left time: 446.0752s\n",
      "loss1 -48.31642532348633 loss2 48.35752487182617\n",
      "\tspeed: 0.4228s/iter; left time: 441.8216s\n",
      "Epoch: 3 cost time: 218.66546845436096\n",
      "Epoch: 3, Steps: 518 | Train Loss: -48.2117809 Vali Loss: -48.1024301 \n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 2.5e-05\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.5317, -1.9877, -0.2659,  ...,  2.2652, -0.7976, -0.4634],\n",
      "         [-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         ...,\n",
      "         [-1.4387, -1.9398, -0.2025,  ...,  2.0419, -0.7802, -0.4634],\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5300, -1.9850, -0.2631,  ...,  2.2171, -0.7976, -0.7881],\n",
      "         [-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.4357, -1.9558, -0.2013,  ...,  2.1828, -0.7627, -0.5284],\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284]],\n",
      "\n",
      "        [[-1.5280, -1.9916, -0.2655,  ...,  2.3773, -0.7802, -1.1128],\n",
      "         [-1.5286, -1.9900, -0.2695,  ...,  2.0076, -0.7802, -0.3985],\n",
      "         [-1.5302, -1.9921, -0.2666,  ...,  2.1652, -0.7976, -0.0089],\n",
      "         ...,\n",
      "         [-1.4287, -1.9644, -0.2040,  ...,  2.0533, -0.7802, -0.5284],\n",
      "         [-1.4280, -1.9613, -0.2011,  ...,  2.0262, -0.7802, -0.5284],\n",
      "         [-1.4231, -1.9653, -0.1961,  ...,  2.1037, -0.7802, -0.5933]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2011, -1.8986, -0.1207,  ...,  2.0282, -0.7627, -0.5284],\n",
      "         [-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         ...,\n",
      "         [-1.0847, -1.8202, -0.0563,  ...,  2.0818, -0.7453, -0.2686],\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881]],\n",
      "\n",
      "        [[-1.2014, -1.8991, -0.1131,  ...,  2.0825, -0.7627, -0.5933],\n",
      "         [-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         ...,\n",
      "         [-1.0816, -1.8346, -0.0607,  ...,  2.1496, -0.7453, -0.6582],\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634]],\n",
      "\n",
      "        [[-1.2018, -1.8986, -0.1278,  ...,  2.2444, -0.7627, -0.4634],\n",
      "         [-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         ...,\n",
      "         [-1.0831, -1.8370, -0.0688,  ...,  1.9231, -0.7802, -0.7881],\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686]]])\n",
      "torch.Size([256, 100, 25])\n",
      "tensor([[[-1.1988, -1.9007, -0.1174,  ...,  2.0516, -0.7802, -0.3985],\n",
      "         [-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0853, -1.8356, -0.0809,  ...,  1.7693, -0.7627, -0.4634],\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284]],\n",
      "\n",
      "        [[-1.1967, -1.9029, -0.1181,  ...,  2.0324, -0.7453, -0.3336],\n",
      "         [-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0867, -1.8257, -0.0683,  ...,  1.9047, -0.7802, -0.2686],\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686]],\n",
      "\n",
      "        [[-1.1974, -1.8984, -0.1123,  ...,  2.1161, -0.7627, -0.5933],\n",
      "         [-1.1956, -1.9019, -0.1047,  ...,  2.1052, -0.7627, -0.5933],\n",
      "         [-1.1970, -1.8963, -0.1063,  ...,  2.1558, -0.7627, -0.5933],\n",
      "         ...,\n",
      "         [-1.0834, -1.8340, -0.0543,  ...,  1.8955, -0.7627, -0.5284],\n",
      "         [-1.0844, -1.8406, -0.0661,  ...,  1.9605, -0.7802, -0.2686],\n",
      "         [-1.0859, -1.8258, -0.0586,  ...,  2.0051, -0.6582, -0.1388]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.9184, -1.7548,  0.0309,  ...,  2.0519, -0.7976,  0.1859],\n",
      "         [-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         ...,\n",
      "         [-0.8101, -1.6940,  0.0503,  ...,  2.1808, -0.7802,  0.9651],\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508]],\n",
      "\n",
      "        [[-0.9141, -1.7619,  0.0301,  ...,  1.9419, -0.7976,  0.2508],\n",
      "         [-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         ...,\n",
      "         [-0.8090, -1.7046,  0.0430,  ...,  2.0193, -0.7802, -0.0738],\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560]],\n",
      "\n",
      "        [[-0.9052, -1.7633,  0.0361,  ...,  2.0399, -0.7976,  0.0560],\n",
      "         [-0.9055, -1.7627,  0.0327,  ...,  1.9180, -0.7802,  0.4456],\n",
      "         [-0.9036, -1.7706,  0.0318,  ...,  1.9065, -0.7279, -0.0738],\n",
      "         ...,\n",
      "         [-0.8087, -1.7079,  0.0471,  ...,  2.2234, -0.7976,  0.2508],\n",
      "         [-0.8103, -1.7083,  0.0430,  ...,  2.3624, -0.7976,  0.0560],\n",
      "         [-0.8081, -1.7113,  0.0315,  ...,  2.1645, -0.7802, -0.2686]]])\n",
      "loss1 -48.23985290527344 loss2 48.29512023925781\n",
      "\tspeed: 5.8502s/iter; left time: 6008.2044s\n",
      "loss1 -48.40700912475586 loss2 48.43812942504883\n",
      "\tspeed: 0.4226s/iter; left time: 429.7905s\n",
      "loss1 -48.369361877441406 loss2 48.429283142089844\n",
      "\tspeed: 0.4225s/iter; left time: 425.4852s\n",
      "loss1 -48.20829772949219 loss2 48.246124267578125\n",
      "\tspeed: 0.4227s/iter; left time: 421.3947s\n",
      "loss1 -48.350196838378906 loss2 48.381629943847656\n",
      "\tspeed: 0.4225s/iter; left time: 417.0394s\n",
      "loss1 -48.343414306640625 loss2 48.374061584472656\n",
      "\tspeed: 0.4226s/iter; left time: 412.8554s\n",
      "loss1 -48.524593353271484 loss2 48.55063247680664\n",
      "\tspeed: 0.4226s/iter; left time: 408.6992s\n",
      "loss1 -48.34173583984375 loss2 48.38182830810547\n",
      "\tspeed: 0.4228s/iter; left time: 404.6334s\n",
      "loss1 -48.26837921142578 loss2 48.306739807128906\n",
      "\tspeed: 0.4228s/iter; left time: 400.3518s\n",
      "loss1 -48.42853927612305 loss2 48.46304702758789\n",
      "\tspeed: 0.4226s/iter; left time: 395.9557s\n",
      "loss1 -48.488975524902344 loss2 48.53150177001953\n",
      "\tspeed: 0.4227s/iter; left time: 391.8408s\n",
      "loss1 -48.46684646606445 loss2 48.49228286743164\n",
      "\tspeed: 0.4226s/iter; left time: 387.5067s\n",
      "loss1 -48.44132995605469 loss2 48.469757080078125\n",
      "\tspeed: 0.4226s/iter; left time: 383.2832s\n",
      "loss1 -48.24852752685547 loss2 48.270790100097656\n",
      "\tspeed: 0.4226s/iter; left time: 379.0527s\n",
      "loss1 -48.50493240356445 loss2 48.60490036010742\n",
      "\tspeed: 0.4225s/iter; left time: 374.7947s\n",
      "loss1 -48.531578063964844 loss2 48.57806396484375\n",
      "\tspeed: 0.4228s/iter; left time: 370.7885s\n",
      "loss1 -48.407737731933594 loss2 48.43699645996094\n",
      "\tspeed: 0.4226s/iter; left time: 366.4090s\n",
      "loss1 -48.30601119995117 loss2 48.41109848022461\n",
      "\tspeed: 0.4226s/iter; left time: 362.2061s\n",
      "loss1 -48.401458740234375 loss2 48.43263244628906\n",
      "\tspeed: 0.4227s/iter; left time: 357.9903s\n",
      "loss1 -48.41289138793945 loss2 48.44894027709961\n",
      "\tspeed: 0.4227s/iter; left time: 353.8140s\n",
      "loss1 -48.355812072753906 loss2 48.38007354736328\n",
      "\tspeed: 0.4226s/iter; left time: 349.5116s\n",
      "loss1 -48.49264144897461 loss2 48.522159576416016\n",
      "\tspeed: 0.4226s/iter; left time: 345.2889s\n",
      "loss1 -48.40703201293945 loss2 48.449398040771484\n",
      "\tspeed: 0.4227s/iter; left time: 341.1307s\n",
      "loss1 -48.42238235473633 loss2 48.449153900146484\n",
      "\tspeed: 0.4227s/iter; left time: 336.8686s\n",
      "loss1 -48.5191650390625 loss2 48.54364013671875\n",
      "\tspeed: 0.4226s/iter; left time: 332.5708s\n",
      "loss1 -48.43698501586914 loss2 48.479434967041016\n",
      "\tspeed: 0.4226s/iter; left time: 328.3741s\n",
      "loss1 -48.39701843261719 loss2 48.535430908203125\n",
      "\tspeed: 0.4227s/iter; left time: 324.1855s\n",
      "loss1 -48.2590446472168 loss2 48.33394241333008\n",
      "\tspeed: 0.4226s/iter; left time: 319.9236s\n",
      "loss1 -48.4616584777832 loss2 48.497196197509766\n",
      "\tspeed: 0.4228s/iter; left time: 315.8101s\n",
      "loss1 -48.1904182434082 loss2 48.2487907409668\n",
      "\tspeed: 0.4226s/iter; left time: 311.4518s\n",
      "loss1 -48.55058288574219 loss2 48.59895324707031\n",
      "\tspeed: 0.4226s/iter; left time: 307.2650s\n",
      "loss1 -48.40028381347656 loss2 48.426918029785156\n",
      "\tspeed: 0.4227s/iter; left time: 303.0556s\n",
      "loss1 -48.55276870727539 loss2 48.58756637573242\n",
      "\tspeed: 0.4228s/iter; left time: 298.9266s\n",
      "loss1 -48.58034133911133 loss2 48.64506912231445\n",
      "\tspeed: 0.4227s/iter; left time: 294.6501s\n",
      "loss1 -48.47202682495117 loss2 48.51864242553711\n",
      "\tspeed: 0.4229s/iter; left time: 290.5552s\n",
      "loss1 -48.52278518676758 loss2 48.58237838745117\n",
      "\tspeed: 0.4229s/iter; left time: 286.2905s\n",
      "loss1 -48.50606155395508 loss2 48.56572341918945\n",
      "\tspeed: 0.4227s/iter; left time: 281.9694s\n",
      "loss1 -48.519412994384766 loss2 48.57831954956055\n",
      "\tspeed: 0.4228s/iter; left time: 277.7515s\n",
      "loss1 -48.38703155517578 loss2 48.44568634033203\n",
      "\tspeed: 0.4228s/iter; left time: 273.5365s\n",
      "loss1 -48.51881408691406 loss2 48.555450439453125\n",
      "\tspeed: 0.4228s/iter; left time: 269.3039s\n",
      "loss1 -48.49846267700195 loss2 48.5352897644043\n",
      "\tspeed: 0.4228s/iter; left time: 265.0658s\n",
      "loss1 -48.19246292114258 loss2 48.29276657104492\n",
      "\tspeed: 0.4228s/iter; left time: 260.8765s\n",
      "loss1 -48.413795471191406 loss2 48.53370666503906\n",
      "\tspeed: 0.4229s/iter; left time: 256.6810s\n",
      "loss1 -48.64011001586914 loss2 48.6960334777832\n",
      "\tspeed: 0.4229s/iter; left time: 252.4610s\n",
      "loss1 -48.58537673950195 loss2 48.634456634521484\n",
      "\tspeed: 0.4228s/iter; left time: 248.2004s\n",
      "loss1 -48.4677848815918 loss2 48.57717514038086\n",
      "\tspeed: 0.4230s/iter; left time: 244.0741s\n",
      "loss1 -48.5993537902832 loss2 48.67478561401367\n",
      "\tspeed: 0.4231s/iter; left time: 239.9258s\n",
      "loss1 -48.42083740234375 loss2 48.531402587890625\n",
      "\tspeed: 0.4230s/iter; left time: 235.6121s\n",
      "loss1 -48.59609603881836 loss2 48.6465950012207\n",
      "\tspeed: 0.4228s/iter; left time: 231.2870s\n",
      "loss1 -48.52595901489258 loss2 48.577091217041016\n",
      "\tspeed: 0.4228s/iter; left time: 227.0217s\n",
      "loss1 -48.43437957763672 loss2 48.474281311035156\n",
      "\tspeed: 0.4225s/iter; left time: 222.6820s\n",
      "Epoch: 4 cost time: 218.5792350769043\n",
      "Epoch: 4, Steps: 518 | Train Loss: -48.4078316 Vali Loss: -48.2789702 \n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Solver at 0x7fa17f3dcb60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6314e350-633a-4a3a-8783-75785d079b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/PSM/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b8d3db-c362-44dc-974d-e482ac830f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_(min)</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>feature_18</th>\n",
       "      <th>feature_19</th>\n",
       "      <th>feature_20</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732689</td>\n",
       "      <td>0.761748</td>\n",
       "      <td>0.606848</td>\n",
       "      <td>0.488746</td>\n",
       "      <td>0.424310</td>\n",
       "      <td>0.403609</td>\n",
       "      <td>0.519318</td>\n",
       "      <td>0.398792</td>\n",
       "      <td>0.451453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.318797</td>\n",
       "      <td>0.451856</td>\n",
       "      <td>0.571500</td>\n",
       "      <td>0.469717</td>\n",
       "      <td>0.609883</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.481838</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.138249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.732799</td>\n",
       "      <td>0.761855</td>\n",
       "      <td>0.607133</td>\n",
       "      <td>0.488781</td>\n",
       "      <td>0.432008</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.402568</td>\n",
       "      <td>0.455657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321463</td>\n",
       "      <td>0.456123</td>\n",
       "      <td>0.562226</td>\n",
       "      <td>0.466533</td>\n",
       "      <td>0.629812</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.477218</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.732938</td>\n",
       "      <td>0.761594</td>\n",
       "      <td>0.606895</td>\n",
       "      <td>0.488791</td>\n",
       "      <td>0.418858</td>\n",
       "      <td>0.407724</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.396526</td>\n",
       "      <td>0.456104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347219</td>\n",
       "      <td>0.456692</td>\n",
       "      <td>0.572002</td>\n",
       "      <td>0.487845</td>\n",
       "      <td>0.643598</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492623</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.092166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.732893</td>\n",
       "      <td>0.761656</td>\n",
       "      <td>0.606478</td>\n",
       "      <td>0.488802</td>\n",
       "      <td>0.417896</td>\n",
       "      <td>0.404242</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.405589</td>\n",
       "      <td>0.460020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361904</td>\n",
       "      <td>0.460532</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.479512</td>\n",
       "      <td>0.644690</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.457064</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.732788</td>\n",
       "      <td>0.761573</td>\n",
       "      <td>0.606777</td>\n",
       "      <td>0.488800</td>\n",
       "      <td>0.421103</td>\n",
       "      <td>0.407407</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.399547</td>\n",
       "      <td>0.458507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359767</td>\n",
       "      <td>0.458825</td>\n",
       "      <td>0.563354</td>\n",
       "      <td>0.448298</td>\n",
       "      <td>0.629948</td>\n",
       "      <td>0.006745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472223</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.170507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132476</th>\n",
       "      <td>132476.0</td>\n",
       "      <td>0.775409</td>\n",
       "      <td>0.909234</td>\n",
       "      <td>0.606076</td>\n",
       "      <td>0.660644</td>\n",
       "      <td>0.445799</td>\n",
       "      <td>0.432099</td>\n",
       "      <td>0.494318</td>\n",
       "      <td>0.435423</td>\n",
       "      <td>0.481634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409246</td>\n",
       "      <td>0.482293</td>\n",
       "      <td>0.585537</td>\n",
       "      <td>0.392673</td>\n",
       "      <td>0.644690</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>0.145742</td>\n",
       "      <td>0.006536</td>\n",
       "      <td>0.110599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132477</th>\n",
       "      <td>132477.0</td>\n",
       "      <td>0.775450</td>\n",
       "      <td>0.909155</td>\n",
       "      <td>0.606434</td>\n",
       "      <td>0.660608</td>\n",
       "      <td>0.446440</td>\n",
       "      <td>0.430199</td>\n",
       "      <td>0.477273</td>\n",
       "      <td>0.436178</td>\n",
       "      <td>0.470712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411880</td>\n",
       "      <td>0.471199</td>\n",
       "      <td>0.591929</td>\n",
       "      <td>0.388539</td>\n",
       "      <td>0.639913</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.150709</td>\n",
       "      <td>0.004357</td>\n",
       "      <td>0.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132478</th>\n",
       "      <td>132478.0</td>\n",
       "      <td>0.775367</td>\n",
       "      <td>0.909096</td>\n",
       "      <td>0.606409</td>\n",
       "      <td>0.660636</td>\n",
       "      <td>0.446119</td>\n",
       "      <td>0.429566</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.434290</td>\n",
       "      <td>0.475673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403343</td>\n",
       "      <td>0.476319</td>\n",
       "      <td>0.593182</td>\n",
       "      <td>0.389498</td>\n",
       "      <td>0.640868</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.152412</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.119816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132479</th>\n",
       "      <td>132479.0</td>\n",
       "      <td>0.775383</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>0.606441</td>\n",
       "      <td>0.660613</td>\n",
       "      <td>0.452213</td>\n",
       "      <td>0.424501</td>\n",
       "      <td>0.482955</td>\n",
       "      <td>0.438444</td>\n",
       "      <td>0.475319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396295</td>\n",
       "      <td>0.476035</td>\n",
       "      <td>0.583532</td>\n",
       "      <td>0.389763</td>\n",
       "      <td>0.639367</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.153409</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.101382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132480</th>\n",
       "      <td>132480.0</td>\n",
       "      <td>0.775374</td>\n",
       "      <td>0.909185</td>\n",
       "      <td>0.606704</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.449968</td>\n",
       "      <td>0.426717</td>\n",
       "      <td>0.471591</td>\n",
       "      <td>0.434668</td>\n",
       "      <td>0.479511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400617</td>\n",
       "      <td>0.480444</td>\n",
       "      <td>0.588670</td>\n",
       "      <td>0.404036</td>\n",
       "      <td>0.638957</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>0.173375</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>0.105991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132481 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp_(min)  feature_0  feature_1  feature_2  feature_3  \\\n",
       "0                   0.0   0.732689   0.761748   0.606848   0.488746   \n",
       "1                   1.0   0.732799   0.761855   0.607133   0.488781   \n",
       "2                   2.0   0.732938   0.761594   0.606895   0.488791   \n",
       "3                   3.0   0.732893   0.761656   0.606478   0.488802   \n",
       "4                   4.0   0.732788   0.761573   0.606777   0.488800   \n",
       "...                 ...        ...        ...        ...        ...   \n",
       "132476         132476.0   0.775409   0.909234   0.606076   0.660644   \n",
       "132477         132477.0   0.775450   0.909155   0.606434   0.660608   \n",
       "132478         132478.0   0.775367   0.909096   0.606409   0.660636   \n",
       "132479         132479.0   0.775383   0.909189   0.606441   0.660613   \n",
       "132480         132480.0   0.775374   0.909185   0.606704   0.660626   \n",
       "\n",
       "        feature_4  feature_5  feature_6  feature_7  feature_8  ...  \\\n",
       "0        0.424310   0.403609   0.519318   0.398792   0.451453  ...   \n",
       "1        0.432008   0.410256   0.511364   0.402568   0.455657  ...   \n",
       "2        0.418858   0.407724   0.488636   0.396526   0.456104  ...   \n",
       "3        0.417896   0.404242   0.500000   0.405589   0.460020  ...   \n",
       "4        0.421103   0.407407   0.511364   0.399547   0.458507  ...   \n",
       "...           ...        ...        ...        ...        ...  ...   \n",
       "132476   0.445799   0.432099   0.494318   0.435423   0.481634  ...   \n",
       "132477   0.446440   0.430199   0.477273   0.436178   0.470712  ...   \n",
       "132478   0.446119   0.429566   0.488636   0.434290   0.475673  ...   \n",
       "132479   0.452213   0.424501   0.482955   0.438444   0.475319  ...   \n",
       "132480   0.449968   0.426717   0.471591   0.434668   0.479511  ...   \n",
       "\n",
       "        feature_15  feature_16  feature_17  feature_18  feature_19  \\\n",
       "0         0.318797    0.451856    0.571500    0.469717    0.609883   \n",
       "1         0.321463    0.456123    0.562226    0.466533    0.629812   \n",
       "2         0.347219    0.456692    0.572002    0.487845    0.643598   \n",
       "3         0.361904    0.460532    0.563354    0.479512    0.644690   \n",
       "4         0.359767    0.458825    0.563354    0.448298    0.629948   \n",
       "...            ...         ...         ...         ...         ...   \n",
       "132476    0.409246    0.482293    0.585537    0.392673    0.644690   \n",
       "132477    0.411880    0.471199    0.591929    0.388539    0.639913   \n",
       "132478    0.403343    0.476319    0.593182    0.389498    0.640868   \n",
       "132479    0.396295    0.476035    0.583532    0.389763    0.639367   \n",
       "132480    0.400617    0.480444    0.588670    0.404036    0.638957   \n",
       "\n",
       "        feature_20  feature_21  feature_22  feature_23  feature_24  \n",
       "0         0.008432    0.000000    0.481838    0.006536    0.138249  \n",
       "1         0.008432    0.000000    0.477218    0.006536    0.115207  \n",
       "2         0.006745    0.000000    0.492623    0.008715    0.092166  \n",
       "3         0.008432    0.000000    0.457064    0.008715    0.142857  \n",
       "4         0.006745    0.000000    0.472223    0.006536    0.170507  \n",
       "...            ...         ...         ...         ...         ...  \n",
       "132476    0.023609    0.039146    0.145742    0.006536    0.110599  \n",
       "132477    0.020236    0.042705    0.150709    0.004357    0.115207  \n",
       "132478    0.020236    0.042705    0.152412    0.008715    0.119816  \n",
       "132479    0.018550    0.042705    0.153409    0.008715    0.101382  \n",
       "132480    0.020236    0.042705    0.173375    0.008715    0.105991  \n",
       "\n",
       "[132481 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a66210c-12d4-4491-98af-b1bc2c4bed60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
